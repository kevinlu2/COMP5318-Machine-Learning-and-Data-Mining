{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining \n",
    "\n",
    "## Tutorial 5 - Classification II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 2, 2020**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To learn about gradient descent. \n",
    "* To learn about the \"linear regression\" classifier.\n",
    "* To learn about the Polynomial Regression,  Overfitting, and Regularization \n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"lab5.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"lab5.ipynb\" file\n",
    "* Complete exercises in \"lab5.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "\n",
    "Lecturers: Nguyen Hoang Tran \n",
    "\n",
    "Tutors: Canh Dinh, Chen Chen, Claire Hardgrove, Fengxiang He, Henry Weld, Yixuan Zhang, Zhiyi Wang, Thomas Selvaraj."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "In general, the aim of **optimization** (a.k.a. mathematical programming) is to determine the,\n",
    "* **minimum of an objective function** i.e. ${\\operatorname{min}}f(x)$ , or\n",
    "* **arguments $x$ that minimizes an objective function** i.e. $\\underset{x}{\\operatorname{argmin}}f(x)$.\n",
    "\n",
    "The objective function is also known as loss, cost, fitness, utility, energy, etc. function. In Machine Learning and Statistics, objectives include determining parameters that maximizes likelihood (ML), minimizes negative log-likelihood (NLL), maximizes a posterior (MAP). \n",
    "\n",
    "\n",
    "#### Convex programming\n",
    "If the function is convex, the local minimum is same as the global minimum. \n",
    "\n",
    "A convex function is defined by,\n",
    "\\begin{equation}\n",
    "    f(\\lambda x_1 + (1-\\lambda) x_2) \\leq \\lambda f(x_1) + (1-\\lambda) f(x_2) \n",
    "\\end{equation}\n",
    "where $\\forall \\lambda \\in [0,1]$, $\\forall x_1, x_2 \\in X$ and $X$ is a convex set.\n",
    "\n",
    "\n",
    "E.g. Let $f(x) := x^2$. Assume,\n",
    "\\begin{equation}\n",
    "    \\big( \\lambda x_1 + (1-\\lambda) x_2 \\big)^2 \\leq \\lambda x_1^2 + (1-\\lambda) x_2^2 \n",
    "\\end{equation}\n",
    "\n",
    "This can be simplified,\n",
    "\n",
    "$\\lambda (1-\\lambda) (x_1 - x_2)^2 \\geq 0 \\implies \\lambda \\in [0, 1] \\forall x_1 \\neq x_2$. \\therefore $f(x) := x^2$ is convex. \n",
    "\n",
    "Read about logarithmically concave functions: https://en.wikipedia.org/wiki/Logarithmically_concave_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the known function $f(x) = p(x-q)^2 + r$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, p=1, q=0, r=0):\n",
    "    return p*(x-q)**2 + r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the function for any $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(-10, 10, 25)\n",
    "yy = f(xx)\n",
    "pl.scatter(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive methods to determine the,\n",
    "* **minimum of the function** i.e. ${\\operatorname{min}}f(x)$ , or\n",
    "* **arguments $x$ that minimizes the function** i.e. $\\underset{x}{\\operatorname{argmin}}f(x)$ are,\n",
    "\n",
    "i) inspecting $f(x)$\n",
    "\n",
    "ii) differentiating $f(x)$ and equating to zero.\n",
    "\n",
    "iii) plotting $f(x)$ vs. $x$ for a finite set of $x$ and inspecting visually. \n",
    "\n",
    "Although we know that $\\underset{x}{\\operatorname{argmin}}f(x) = q$ for this simple function, evaluating $\\operatorname{argmin} f$ is not straightforward in many practical scenarios. Therefore, for demonstration purposes, let us assume that we have no clue about the minimum of this function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1:** Evaluate the following when $x=8$. \n",
    "* function i.e. $f(x=8)$\n",
    "* gradient i.e. $f'(x)\\rvert_{x=8}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 8\n",
    "f_x0 = f(x0)\n",
    "\n",
    "delta = 1e-2\n",
    "f_x0_plus_delta = f(x0 + delta)\n",
    "\n",
    "#Gradient measures how much the output of a function changes if you change the inputs a little bit\n",
    "gradient = (f_x0_plus_delta - f_x0)/delta #(change in y or f(x))/(change in x)\n",
    "\n",
    "print('f(x={})={} and f\\'(x)_(x={})={}'.format(x0, f_x0, x0, gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us develop the gradient descent algorithm. The naive idea is, \n",
    "* randomly start anywhere in the curve\n",
    "* iteratively, move towards the direction of steepest descent (i.e. gradient f'(x)) by taking little steps\n",
    "\\begin{equation}\n",
    "    x \\leftarrow x - \\eta \\cdot f'(x)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GD(): \n",
    "    def __init__(self, eta=0.1, x0=0, max_iter=50, diff_to_stop=0.01, delta=0.01): \n",
    "        self.x0 = x0 #randomly initialize any value\n",
    "        self.delta = delta #used for gradient calculations\n",
    "        self.eta = eta #learning rate \n",
    "        self.diff_to_stop = diff_to_stop #stop the algorithms if steps are smaller than this value\n",
    "        self.max_iter = max_iter #when to stop\n",
    "\n",
    "    def run(self, f, plot_method=None):       \n",
    "        pl.figure(figsize=(8, 4))\n",
    "        xx = np.linspace(-10, 10, 100)\n",
    "        yy = f(xx)\n",
    "        pl.plot(xx, yy)\n",
    "        pl.grid()\n",
    "        \n",
    "        x_new = self.x0\n",
    "        iter_no = 0\n",
    "        while True:\n",
    "            iter_no += 1\n",
    "\n",
    "            #main algorithm\n",
    "            x = x_new #note: x_new is the position of the previous move\n",
    "            grad = (f(x+self.delta) - f(x))/self.delta #evaluate the gradient at f(x)\n",
    "            x_new = x - self.eta*grad #move in the direction of gradient\n",
    "            #end of main algorithm\n",
    "\n",
    "            if plot_method is 'scatter':\n",
    "                pl.scatter(x_new, f(x_new), color='r')\n",
    "            else:\n",
    "                pl.arrow(x, f(x), x_new-x, f(x_new)-f(x), head_width=0.3, head_length=2, color='r')\n",
    "\n",
    "            step_size = np.abs(x_new - x)\n",
    "            if step_size <= self.diff_to_stop:\n",
    "                print('Successfully converged with a step size of {} after {} iterations!'.format(step_size, iter_no))\n",
    "                pl.scatter(x_new, f(x_new), s=200, marker='*')\n",
    "                break\n",
    "            elif iter_no == self.max_iter:\n",
    "                print('Max iterations completed! Convergence cannot be guaranteed. Step size={}'.format(step_size))\n",
    "                pl.scatter(x_new, f(x_new), s=200, marker='*')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, p=1, q=0, r=0):\n",
    "    return p*(x-q)**2 + r\n",
    "\n",
    "GD(eta=0.1, x0=8, max_iter=5, diff_to_stop=0.01, delta=0.01).run(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2:** \n",
    "* Increase the maximum number of iterations in order to 50.\n",
    "* Change the learning rate to different values i.e. $\\eta \\in \\{0.05, 0.1, 0.8, 1.1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order for Gradient Descent to reach the local minimum, we have to set the learning rate to an appropriate value,\n",
    "#which is neither too low nor too high. This is because if the steps it takes are too big, it maybe will not reach the\n",
    "#local minimum because it just bounces back and forth between the convex function of gradient descent like you can see\n",
    "#on the output plot below. If you set the learning rate to a very small value, gradient descent will eventually reach\n",
    "#the local minimum but it will maybe take too much time.\n",
    "ex12 = GD(eta=0.8, x0=8, max_iter=50, diff_to_stop=0.01, delta=0.01)\n",
    "ex12.run(f)\n",
    "\n",
    "#Change p to 0.3 and observe the output for eta={0.9, 1.0, 1.1}\n",
    "def f2(x, p=0.3, q=0, r=0):\n",
    "    return p*(x-q)**2 + r\n",
    "ex12 = GD(eta=0.9, x0=8, max_iter=50, diff_to_stop=0.01, delta=0.01).run(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.3:** \n",
    "Can you determine $\\underset{x}{\\operatorname{argmin}}f(x)$ where $f(x) = -\\exp(-(x-2)^2) - 0.5 \\exp(-(x+2)^2)$?\n",
    "\n",
    "\n",
    "Suggestion: set max_iter to $100$, x0 to $-3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "def f_multimodal(x):\n",
    "    return -np.exp(-(x-2)**2) - 0.5*np.exp(-(x+2)**2)\n",
    "\n",
    "GD(eta=0.1, x0=-3, max_iter=100, diff_to_stop=0.01, delta=0.01).run(f_multimodal, plot_method='scatter')\n",
    "\n",
    "#Sptops at local minimum. Read about momentum in gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read http://scikit-learn.org/stable/modules/sgd.html#sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 House Prices Dataset\n",
    "**Dataset descriptions:**\n",
    "* train.csv - the training set\n",
    "* test.csv - the test set\n",
    "* data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "print(os.listdir(\"./data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the libraries and data...\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "data = pd.read_csv('./data/train.csv')\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this tutorial we only consider one feature the Living Area to predict the Sale Price with the expectation that the Price will increase when the Living Area increases.\n",
    "However we can use more features like the number of bedroom...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = data[['GrLivArea','SalePrice']]\n",
    "print(newData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "%matplotlib inline\n",
    "#Grab the relevant data, scale the predictor variable, and add a column of 1s for the gradient descent...\n",
    "x = newData['GrLivArea']\n",
    "y = newData['SalePrice']\n",
    "x = (x - x.mean()) / x.std()\n",
    "print(x.shape)\n",
    "x = np.c_[np.ones(x.shape[0]), x] \n",
    "print(x.shape)\n",
    "pl.scatter(x[:,1], y,marker='.', color = 'r', label = 'Training samples')\n",
    "pl.xlabel('Living Area in square feet (normalised)')\n",
    "pl.ylabel('Sale Price ($)')\n",
    "pl.legend(loc='lower right')\n",
    "plt.title('Sale Price vs Living Area')\n",
    "pl.grid()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building Linear Regression model using Gradient Decent with sum square error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Linear Model:\n",
    "* input: $x \\in R^D$ (covariates, predictors, features, etc)\n",
    "* Output: $y \\in R$ (responses, targets, outcomes, outputs, etc)\n",
    "* Model: $f: x \\to y$,with $f(x)=w_0+ \\\n",
    "\\sum_{d=1}^{D}w_dx_d =w_0+w^T x.$\n",
    "\n",
    "Minimize the Residual sum of squares:\n",
    "$ RSS(w) = \\sum_{n=1}^{N}[y_n-f(x_n)]^2 =  \\sum_{n=1}^{N}[y_n- (w_0 + \\sum_{d=1}^{D}w_dx_{nd}) ]^2 .$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Gradient decent:\n",
    "* Initialize $w =  w^{(0)}$ randomly:\n",
    "* Choose learning rate $ \\eta> 0$\n",
    "*  Loop until convergence:\n",
    "\n",
    "    Compute Gradient: $\\nabla RSS(w) = X^T(Xw^t-y)$\n",
    "    \n",
    "    Update parameters: $w^{t+1} = w^t - \\eta * \\nabla RSS(w)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADIENT DESCENT\n",
    "def gradient_descent(x, y, w, iterations, eta):\n",
    "    past_loss = []\n",
    "    past_w = [w]\n",
    "    n = y.size\n",
    "    for i in range(iterations):\n",
    "        prediction = np.dot(x, w)\n",
    "        error = prediction - y\n",
    "        loss = # TODO define loss function\n",
    "        past_loss.append(loss)\n",
    "        \n",
    "        GradRss = np.dot(x.T, error)\n",
    "        w = # TODO Defind update rule for w\n",
    "        past_w.append(w)\n",
    "        \n",
    "    return past_w, past_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the relevant variables to the function and get the new values back...\n",
    "eta = 0.01 #Step size\n",
    "iterations = 2000 #No. of iterations\n",
    "np.random.seed(123) #Set the seed\n",
    "w0 = np.random.rand(2) #Pick some random values to start with\n",
    "\n",
    "past_w, past_loss = gradient_descent(x, y, w0, iterations, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the cost function...\n",
    "plt.title('Loss Function')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(past_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 The changing of model through each interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Animation\n",
    "#Set the plot up,\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.title('Sale Price vs Living Area')\n",
    "plt.xlabel('Living Area in square feet (normalised)')\n",
    "plt.ylabel('Sale Price ($)')\n",
    "plt.scatter(x[:,1], y, color='red')\n",
    "line, = ax.plot([], [], lw=2)\n",
    "annotation = ax.text(-1, 700000, '')\n",
    "annotation.set_animated(True)\n",
    "plt.close()\n",
    "\n",
    "#Generate the animation data,\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    annotation.set_text('')\n",
    "    return line, annotation\n",
    "\n",
    "# animation function.  This is called sequentially\n",
    "def animate(i):\n",
    "    x = np.linspace(-5, 20, 1000)\n",
    "    y = past_w[i][1]*x + past_w[i][0]\n",
    "    line.set_data(x, y)\n",
    "    annotation.set_text('loss = %.2f e10' % (past_loss[i]/10000000000))\n",
    "    return line, annotation\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=300, interval=0, blit=True)\n",
    "\n",
    "anim.save('animation.gif', writer='imagemagick', fps = 30)\n",
    "#Display the animation...\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "filename = 'animation.gif'\n",
    "\n",
    "video = io.open(filename, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<img src=\"data:image/gif;base64,{0}\" type=\"gif\" />'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Polynomial Regression, Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x), and has been used to describe nonlinear phenomena such as the growth rate of tissues,[1] the distribution of carbon isotopes in lake sediments,[2] and the progression of disease epidemi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Objective***: Estimate (a.k.a. predict or query) outputs for unknown inputs, given the training data $\\{\\mathbf{x},\\mathbf{y}\\}$ with datasize $n$.\n",
    "\n",
    "We can use a nonlinear mapping:\n",
    " \\begin{equation}\n",
    "    \\phi(x): x \\in R^D \\leftarrow z \\in R^M \n",
    "\\end{equation}\n",
    "$\\phi(x)$ is a polynomial feature matrix\n",
    "* M is dimensionality of new features z (or $\\Phi(x)$)\n",
    "* M could be greater than, less than, or equal to D\n",
    "Consider the non linear model with:\n",
    "\\begin{equation}\n",
    "\\phi(x) = \\begin{bmatrix}\n",
    "1\\\\\n",
    "x\\\\\n",
    "x^2\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "x^M\\\\\n",
    "\\end{bmatrix} \n",
    "\\rightarrow  f(x)= w^T \\phi(x) = w_0 +  w_1 x + w_2 x^2 + w_3 x^3 + ... + w_m x^m = w_0 + \\sum_{m=1}^M w_mx^m\n",
    "\\end{equation}\n",
    "\n",
    "Residual sum of squares:\n",
    "\\begin{equation}\n",
    "\\sum_n[w^T \\phi(x_n)-y_n]^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The LMS solution can be formulated with the new design matrix:\n",
    "\\begin{equation}\n",
    "  \\mathbf{\\Phi} = \\begin{bmatrix}\n",
    "  \\phi(x_1)^T\\\\\n",
    "  \\phi(x_2)^T\\\\\n",
    "  .\\\\\n",
    "  .\\\\\n",
    "  .\\\\\n",
    "  \\phi(x_N)^T\n",
    "  \\end{bmatrix} =  \\begin{bmatrix}\n",
    "  x_1^0 & x_1^1 & \\cdots & x_1^m \\\\\n",
    "  x_2^0 & x_2^1 & \\cdots & x_2^m \\\\\n",
    "  x_3^0 & x_3^1 & \\cdots & x_3^m \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  x_N^0 & x_N^1 & \\cdots & x_N^m\n",
    " \\end{bmatrix} \\in R^{NxM},  \\quad \n",
    "  \\mathbf{w^{LMS}} = (\\mathbf{\\Phi}^{\\top}\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi^{\\top} y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate nonlinear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nTrain = 30\n",
    "nTest = 20\n",
    "np.random.seed(1)\n",
    "xTrain = np.random.rand(nTrain, 1)*5\n",
    "yTrain = 3*(xTrain -2) * (xTrain - 3)*(xTrain-4) +  10 * np.random.randn(nTrain, 1)\n",
    "xTest = (np.random.rand(nTest,1) -1/8) *8\n",
    "yTest = 3*(xTest -2) * (xTest - 3)*(xTest-4) +  10*np.random.randn(nTest, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "%matplotlib inline\n",
    "pl.scatter(xTrain, yTrain,marker='.', color = 'b',label = 'Training samples')\n",
    "pl.scatter(xTest, yTest, marker='*', color='r',label = 'Testing samples')\n",
    "pl.xlabel('x')\n",
    "pl.ylabel('y')\n",
    "pl.legend(loc='lower right')\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardData(z, mean, std): \n",
    "    z_standard = (z - mean)/ std\n",
    "    return z_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain_n = standardData(xTrain,xTrain.mean(),xTrain.std())\n",
    "yTrain_n = standardData(yTrain,yTrain.mean(),yTrain.std())\n",
    "xTest_n  = standardData(xTest,xTrain.mean(),xTrain.std())\n",
    "yTest_n  = standardData(yTest,yTrain.mean(),yTrain.std())\n",
    "bias = 0.01\n",
    "xMin = xTrain_n.min() - bias\n",
    "xMax = xTrain_n.max() + bias\n",
    "if(xTrain_n.min() > xTest_n.min()):\n",
    "    xMin = xTest_n.min() - bias\n",
    "if(xTrain_n.max() < xTest_n.max()):\n",
    "    xMax = xTest_n.max() + bias\n",
    "\n",
    "pl.scatter(xTrain_n, yTrain_n,marker='.', color = 'b',label = 'Training samples')\n",
    "pl.scatter(xTest_n, yTest_n, marker='*', color='r',label = 'Testing samples')\n",
    "pl.xlabel('x')\n",
    "pl.ylabel('y')\n",
    "pl.legend(loc='lower right')\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Python method to generate the polynomial feature matrix. It will be used to transform $\\mathbf{x}$ to $\\mathbf{\\phi(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_mat(x_in, m):\n",
    "    # generate (Nxp matrix) feature matrix.\n",
    "    # TODO:\n",
    "    return ϕ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train:** Evaluate $\\phi(x)$ and hence determine the best weights $\\mathbf{w^{LMS}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "#print(xTrain_n)\n",
    "ϕ_x = generate_feature_mat(xTrain_n, m)\n",
    "\n",
    "#TODO:\n",
    "\"\"\"\n",
    "Attention! - use np.linalg.pinv instead of np.linalg.inv. \n",
    "This is because, when a matrix is singular, we may have to obtain the Moore-Penrose pseudoinverse,\n",
    "especially when the polynomial order is very high. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 100 query inputs $\\mathbf{x}_q$ from xMin,xMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_q = np.linspace(xMin,xMax,100)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict:** \n",
    "Evaluate $\\mathbf{\\phi(x_q)}$ and hence determine outputs $\\hat{\\mathbf{y}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϕ_x_q = generate_feature_mat(x_q, m) #generate feature matrix for x_q\n",
    "y_hat = ϕ_x_q.dot(w_lms) #estimated outputs (N_qx1 matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scatter(xTrain_n, yTrain_n,marker='.', color = 'b',label = 'Training samples')\n",
    "pl.scatter(xTest_n, yTest_n, marker='*', color='r',label = 'Testing samples')\n",
    "pl.plot(x_q, y_hat,label= \"m =\" + str(m))\n",
    "pl.xlabel('x')\n",
    "pl.ylabel('y')\n",
    "pl.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1**: Perform regression for $m \\in \\{0, 1, 3, 4, 10\\}$. Identify the models that **under-fit** and **over-fit**. Discuss how to choose a suitable model complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vals = np.array([1,3,4,10])\n",
    "pl.figure(figsize=(5,5))\n",
    "pl.scatter(xTrain_n, yTrain_n, marker='.', color='b')\n",
    "pl.scatter(xTest_n, yTest_n, marker='*', color='r')\n",
    "for m in m_vals:\n",
    "    ϕ_x = generate_feature_mat(xTrain_n,m) \n",
    "    w_lms = (np.linalg.pinv(ϕ_x.T.dot(ϕ_x))).dot(ϕ_x.T.dot(yTrain_n))\n",
    "    ϕ_x_q = generate_feature_mat(x_q, m)\n",
    "    y_hat = ϕ_x_q.dot(w_lms)\n",
    "    pl.plot(x_q, y_hat, label='m={}'.format(m))\n",
    "pl.legend(loc='lower right')\n",
    "pl.ylim([-4,8])\n",
    "pl.title(\"m =1: Underfitting, m = 3: Good fit, m = 10: Overfitting\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tikhonov ($\\mathrm{L}^2$) regularization can be used to overcome overfitting. \n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{w^{LMS}} = (\\mathbf{\\Phi}^{\\top}\\mathbf{\\Phi} + \\lambda \\mathbf{I})^{-1}\\mathbf{\\Phi^{\\top} y}\n",
    "\\end{equation}\n",
    "\n",
    "In other words, we can cosider this as a minimization problem in the following form.\n",
    "\\begin{equation}\n",
    " \\mathbf{w^{LMS}} = \\underset{\\mathbf{w}}{\\operatorname{argmin}} \\| \\mathbf{w^T\\phi(x) - y}\\|_2^2\n",
    " + \\lambda \\| \\mathbf{w} \\|_2^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2** : Define the loss function between real output and predicted output based on Residual sum of squares function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def loss(yPre,y):\n",
    "    loss = mean_squared_error(y,yPre)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3**: Test the following code for different values of the regularization parameter $\\lambda$ and obseve the changing of traning loss and testing loss. Choose the best value for regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_val = np.linspace(0,10,100)[:,np.newaxis]\n",
    "m = 3\n",
    "trainLoss = []\n",
    "testLoss = []\n",
    "for lamb in lambda_val:\n",
    "    ϕ_x_train = generate_feature_mat(xTrain_n, m)\n",
    "    ϕ_x_test = generate_feature_mat(xTest_n, m)\n",
    "    w_lms = (np.linalg.pinv(ϕ_x_train.T.dot(ϕ_x_train)+lamb*np.eye(ϕ_x_train.shape[1]))).dot(ϕ_x_train.T.dot(yTrain_n))\n",
    "    trainLoss.append(loss(ϕ_x_train.dot(w_lms),yTrain_n))\n",
    "    testLoss .append(loss(ϕ_x_test.dot(w_lms),yTest_n))\n",
    "pl.plot(lambda_val, trainLoss, marker='.', color='b',label = \"Training Loss\")\n",
    "pl.plot(lambda_val, testLoss, marker='.', color='r',label = \"Test Loss\")\n",
    "pl.legend()\n",
    "pl.xlabel('Lambda')\n",
    "pl.ylabel('Loss')\n",
    "pl.show()\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
