{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining \n",
    "\n",
    "## Tutorial 6 - Logistic regession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 2, 2020**\n",
    "\n",
    "**Objectives:**\n",
    "* To learn about the \"logistic regression\" classifier.\n",
    "* To learn about the using \"logistic regression\" classifier for MNIST dataset.\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"lab6.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"lab6.ipynb\" file\n",
    "* Complete exercises in \"lab6.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "\n",
    "Lecturers: Nguyen Hoang Tran \n",
    "\n",
    "Tutors: Canh Dinh, Chen Chen, Claire Hardgrove, Fengxiang He, Henry Weld, Yixuan Zhang, Zhiyi Wang, Thomas Selvaraj."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider an $D$ dimensional input array $\\mathbf{x}_{\\text{orig}}$ and a corresponding output array $\\mathbf{y}$. \n",
    "\\begin{equation}\n",
    "   \\mathbf{x}_{\\text{orig}} = \\left( \n",
    "    \\begin{array}{cccc}\n",
    "        x_{11} & x_{12} & \\cdots & x_{1D} \\\\\n",
    "        x_{21} & x_{22} & \\cdots & x_{2D} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        x_{N1} & x_{N2} & \\cdots & x_{ND} \\\\\n",
    "     \\end{array} \n",
    "     \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{y} = \\left( \n",
    "    \\begin{array}{c}\n",
    "        y_{1}  \\\\\n",
    "        y_{2}  \\\\\n",
    "        \\vdots \\\\\n",
    "        y_{N}  \\\\\n",
    "     \\end{array} \n",
    "     \\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = np.array([1.0, 1.8, 2.1, 3.8, 4.7, 6.0, 5.7, 6.9, 7.6, 9.3])[:, np.newaxis] #input - column vector\n",
    "y = np.array([0]*6 + [1]*4)[:, np.newaxis] #output - column vector\n",
    "\n",
    "pl.scatter(X_orig, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us augment the input matrix with a $1 \\times N$ constant array,\n",
    "\\begin{equation}\n",
    "   \\mathbf{x} = \\left( \n",
    "    \\begin{array}{ccccc}\n",
    "        \\color{blue}{1} & x_{11} & x_{12} & \\cdots & x_{1D} \\\\\n",
    "        \\color{blue}{1} & x_{21} & x_{22} & \\cdots & x_{2D} \\\\\n",
    "        \\color{blue}{\\vdots} & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        \\color{blue}{1} & x_{N1} & x_{N2} & \\cdots & x_{ND} \\\\\n",
    "     \\end{array} \n",
    "     \\right)_{\\big(N \\times \\color{red}{(D+1)} \\big)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_to_intercept(x):\n",
    "    return np.hstack((np.ones((x.shape[0],1)), x))\n",
    "\n",
    "X = extend_to_intercept(X_orig)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By considering an arbitrary weight vector,\n",
    "\\begin{equation}\n",
    "   \\mathbf{w} = \\left( \n",
    "    \\begin{array}{c}\n",
    "        w_{1}  \\\\\n",
    "        w_{2}  \\\\\n",
    "        \\vdots \\\\\n",
    "        w_{\\color{red}{D+1}}  \\\\\n",
    "     \\end{array} \n",
    "     \\right)\n",
    "\\end{equation},\n",
    "\n",
    "it is possible to build a linear model, \n",
    "\n",
    "\\begin{equation}\n",
    " \\color{blue}{ z_i =  \\mathbf{x}_i \\cdot \\mathbf{w}_i } = w_0 + x_{i1} w_1 + x_{i2} w_2 + \\cdots + x_{i(D+1)} w_{i(D+1)}\n",
    "\\end{equation}\n",
    "for $i={1,...,N}$\n",
    "\n",
    "Let us equate the linear model to a probability $p(x)$ with logistic transformation applied.\n",
    "\\begin{equation}\n",
    "    \\log \\Big( \\frac{p(\\mathbf{x}_i)}{1-p(\\mathbf{x}_i)} \\Big) = \\mathbf{x}_i \\cdot \\mathbf{w}_i  = z_i\n",
    "\\end{equation}\n",
    "\n",
    "Therefore,\n",
    "\\begin{equation}\n",
    "    p(\\mathbf{x}_i; w) = \\frac{e^{z_i}}{1+e^{z_i}}\n",
    "            = \\frac{1}{1+e^{-z_i}}\n",
    "\\end{equation}\n",
    "\n",
    "Let $Y_i | X=\\mathbf{x}_i \\sim \\mathcal{Bernoulli}(p_i) $\n",
    "\n",
    "Let this probability be,\n",
    "\\begin{equation}\n",
    "    Pr(Y_i=y_i|\\mathbf{x}_i)= \n",
    "    \\begin{cases}\n",
    "        p_i,& \\text{if } y_i = 1\\\\\n",
    "        1-p_i , & \\text{otherwise i.e. } (y_i = 0)\n",
    "    \\end{cases}\n",
    "    = p_i^y (1-p_i)^{1-y_i}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\t\n",
    "[//]: # (COMMENT: Considering success probabilities of all trials $p(\\mathbf{x}_i)$ are the same, $p_i (\\mathbf{x}) := p(\\mathbf{x}_i) \\forall i$. Note that, assuming all observations are independent $\\mathbb{E}[Y_i | [//]: [//]: <> \\mathbf{x}_i] = #p(\\mathbf{x})$ )\n",
    "\n",
    "Considering independence, the likelihood can be written as,\n",
    "\\begin{equation}\n",
    "    L(\\mathbf{w}) = \\prod_{i=1}^{N} p_i^{y_i} \\big( 1 - p_i\\big) ^ {(1-y_i)}\n",
    "\\end{equation}\n",
    "\n",
    "Taking log, the log-likelihood can be written as,\n",
    "\\begin{equation}\n",
    "    l(\\mathbf{w}) = \\sum_{i=1}^{N} \\bigg( y_i log \\big( p(\\mathbf{x_i}) \\big) +\n",
    "          (1-y_i) \\log \\big( 1 - p(\\mathbf{x_i}) \\big)  \\bigg)\n",
    "\\end{equation}\n",
    "\n",
    "This can be simplified to,\n",
    "\\begin{equation}\n",
    "\\color{blue}{\n",
    "    \\text{loss}(\\mathbf{w}) = -l(\\mathbf{w}) = \\sum_{i=1}^{N} \\big( \\log(1+e^z) - y_i z_i \\big) \n",
    "    }\n",
    "\\end{equation}\n",
    "\n",
    "Our objective is to determine $\\mathbf{w}$ that minimizes the overall loss.\n",
    "\\begin{equation}\n",
    "    \\underset{\\mathbf{w}}{\\operatorname{argmin}} \\text{loss}(\\mathbf{w}) \n",
    "    = \\underset{\\mathbf{w}}{\\operatorname{argmin}} \\sum_{i=1}^{N} \\big( \\log(1+e^z) - y_i z_i \\big) \n",
    "\\end{equation}\n",
    "\n",
    "Note: Maximizing log-likelihood is equivalent to minimizing negative-log-likelihood. We would rather like to consider the minimization problem as most of the python optimization packages prefer minimization problems. \n",
    "\n",
    "Refer: http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: \n",
    "\n",
    "The naive approach would be performing a **grid search**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1:** Evaluate the loss function for five-hundred $w_0$-$w_1$ pairs and hence determine $\\underset{w_0, w_1}{\\operatorname{argmax}} \\text{loss}(w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate w0-w1 pairs\n",
    "ww0, ww1 = np.meshgrid(np.linspace(-15, 5, 25), np.linspace(-1, 2, 25))\n",
    "ww = np.vstack((ww0.ravel(), ww1.ravel()))\n",
    "\n",
    "#evaluate the loss function for all ww values\n",
    "z = np.dot(X, ww)\n",
    "exp_z = np.exp(z)\n",
    "obj_fn_vals = # TODO\n",
    "\n",
    "#argmin\n",
    "ww_argmin = ww[:, np.argmin(obj_fn_vals)]\n",
    "\n",
    "#logistic function\n",
    "def logistic_predictions(weights, inputs):\n",
    "    z = np.dot(inputs, weights)\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "#generate query inputs\n",
    "Xq_orig = np.arange(0, 10, 0.1)[:, np.newaxis]\n",
    "Xq = extend_to_intercept(Xq_orig)\n",
    "\n",
    "#evaluate outputs for query inputs\n",
    "yq = logistic_predictions(ww_argmin, Xq)\n",
    "\n",
    "#plot\n",
    "pl.figure(figsize=(15, 5))\n",
    "pl.subplot(121)\n",
    "pl.scatter(ww[0, :], ww[1, :], c=obj_fn_vals)\n",
    "pl.colorbar(); pl.axis('tight'); pl.title('loss function - grid search'); pl.xlabel('w0'); pl.ylabel('w1')\n",
    "pl.subplot(122)\n",
    "pl.scatter(X_orig, y, label='input')\n",
    "pl.plot(Xq_orig, yq, 'r-', label='LR')\n",
    "pl.plot(Xq_orig, np.round(yq, 0), 'r--', label='LR binarized')\n",
    "pl.grid(); pl.title('logistic(w0 + w1*x) model'); pl.xlabel('input x'); pl.ylabel('output y'); pl.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2:** Discuss the disadvantages of grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2:\n",
    "\n",
    "Let us try to evaluate $\\underset{w_0, w_1}{\\operatorname{argmin}} \\text{loss}(w)$ using the **gradient descent algorithm**. We can analytically differentiate the loss function. \n",
    "\n",
    "The derivative w.r.t. each weight is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\color{blue}{\n",
    "    \\frac{\\partial}{\\partial w_d} \\text{loss}(w) =  \\sum_{i=1}^{N} \\bigg( \\frac{e^z}{1+e^z} -y_i \\bigg) x_{id}  \n",
    "    }\n",
    "\\end{equation}\n",
    "\n",
    "for $d \\in \\{1,2,...,(D+1)\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss_and_grad(weights, inputs, targets):\n",
    "\n",
    "    #derivative of the loss function\n",
    "    z = np.dot(inputs, weights)\n",
    "    exp_z = np.exp(z)\n",
    "    dloss = # TODO\n",
    "\n",
    "    #calculating the loss is optional\n",
    "    loss = -np.sum(-np.log(1.0 + exp_z) + targets*z)\n",
    "\n",
    "    return dloss, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us re-implement the gradient descent algorithm (as it is just 2 lines of code). The update rule is,\n",
    "\\begin{equation}\n",
    "    w_d \\leftarrow w_d - \\eta \\cdot \\frac{\\partial}{\\partial w_d} \\text{loss}(w)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = X.shape\n",
    "weights = np.random.random(D)[:, np.newaxis]#1*np.ones(D)[:, np.newaxis]\n",
    "\n",
    "eta = 0.03\n",
    "n_iter = 2000\n",
    "\n",
    "loss_arr = np.empty((1, n_iter)) #for plotting purposes\n",
    "weights_arr = np.empty((n_iter, D)) #for plotting purposes\n",
    "for epoch in range(n_iter):\n",
    "    #save weights for plotting purposes\n",
    "    weights_arr[epoch, :] = weights.ravel()\n",
    "    \n",
    "    #gradient descent\n",
    "    dloss, loss = logistic_loss_and_grad(weights, X, y)\n",
    "    weights = # TODO\n",
    "\n",
    "    #save loss for plotting purposes\n",
    "    loss_arr[0, epoch] = loss\n",
    "\n",
    "#plot\n",
    "Xq = np.arange(0, 10, 0.1)[:, np.newaxis]\n",
    "Xq = extend_to_intercept(Xq)\n",
    "yq = logistic_predictions(weights, Xq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(15, 15))\n",
    "pl.subplot(221)\n",
    "pl.scatter(X[:, 1].ravel(), y, label='inputs')\n",
    "pl.plot(Xq[:, 1].ravel(), yq, 'r-', label='LR')\n",
    "pl.plot(Xq[:, 1].ravel(), np.round(yq, 0), 'r--', label='LR binarized')\n",
    "pl.grid(); pl.title('logistic({} + {}*x) model'.format(np.round(weights[0,:],2), np.round(weights[1,:],2))); \n",
    "pl.xlabel('input x'); pl.ylabel('output y'); pl.legend(loc='lower right')\n",
    "\n",
    "pl.subplot(223)\n",
    "pl.plot(np.arange(n_iter), loss_arr.ravel())\n",
    "pl.grid(); pl.title('learning curve for learning rate = {}'.format(eta)); pl.xlabel('epoch'); pl.ylabel('loss')\n",
    "\n",
    "pl.subplot(224)\n",
    "pl.contourf(ww0, ww1, obj_fn_vals.reshape(ww0.shape[0], ww0.shape[1]), 100); pl.colorbar()\n",
    "pl.contour(ww0, ww1, obj_fn_vals.reshape(ww0.shape[0], ww0.shape[1]), 20, colors='k', hold='on')\n",
    "pl.plot(weights_arr[:, 0], weights_arr[:, 1], c='w')\n",
    "pl.grid(); pl.title('loss function'); pl.xlabel('w0'); pl.ylabel('w1')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.3:** Change the learning rate $\\eta$ and number of iterations and observe results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.4:** Implement L-2 regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply logistic regression for MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784')\n",
    "N, d = mnist.data.shape\n",
    "print('Total {:d} digits, each has {:d} pixels.'.format(N, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider subset of MNIST, only digit 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = mnist.data \n",
    "y_all = mnist.target\n",
    "\n",
    "X0 = X_all[np.where(y_all == '0')[0]] # all digit 0\n",
    "X1 = X_all[np.where(y_all == '1')[0]] # all digit 1\n",
    "y0 = np.zeros(X0.shape[0]) # class 0 label \n",
    "y1 = np.ones(X1.shape[0])  # class 1 label\n",
    "\n",
    "X = np.concatenate((X0, X1), axis = 0) # all digits\n",
    "y = np.concatenate((y0, y1)) # all labels \n",
    "#split train and test (test size = 2000)\n",
    "X_train, X_test, y_train, y_test = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = #TODO: get logistic model and train\n",
    "\n",
    "#evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy %.2f %%\" % (100*accuracy_score(y_test, y_pred.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
