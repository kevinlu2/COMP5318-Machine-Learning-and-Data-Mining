{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining \n",
    "\n",
    "## Tutorial 7 - Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 2, 2020**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To learn about Support Vector Machines.\n",
    "\n",
    "**Instructions:**\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"lab7.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"lab7.ipynb\" file\n",
    "* Complete exercises in \"lab7.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "\n",
    "Lecturers: Nguyen Hoang Tran \n",
    "\n",
    "Tutors: Canh Dinh, Chen Chen, Claire Hardgrove, Fengxiang He, Henry Weld, Yixuan Zhang, Zhiyi Wang, Thomas Selvaraj."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "np.random.seed(22)\n",
    "\n",
    "means = [[2, 2], [4, 2]]\n",
    "cov = [[.3, 2], [.2, .3]]\n",
    "N = 20\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N) # each row is a data point \n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "X = np.concatenate((X0, X1))\n",
    "y = np.concatenate((np.ones(N), -np.ones(N)))\n",
    "\n",
    "plt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = 1)\n",
    "plt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = 1)\n",
    "plt.axis('equal')\n",
    "plt.ylim(0, 4)\n",
    "plt.xlim(2, 4)\n",
    "plt.xlabel('$x_1$', fontsize = 20)\n",
    "plt.ylabel('$x_2$', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Solving SVM problem using Hinge Loss Function and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Hinge Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of Softmax function:\n",
    "Input: $\\textbf{x} \\in \\textbf{R}^{D}$\n",
    "\n",
    "$$\n",
    "\\textbf{J}(w,b) = min_{\\textbf{w},b} \\sum_n max(0,1 - y_n [ \\textbf{w}^T\\textbf{x}_n +b ]) + \\frac{\\lambda}{2} ||\\textbf{w}||^2 \n",
    "$$\n",
    "\n",
    "Can solve using gradient descent to get the optimal $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gradient of hinge loss function:\n",
    "\n",
    "if $1-y_n[ \\textbf{w}^T\\textbf{x}_n +b] \\geq 0$:\n",
    "$$\n",
    "\\nabla \\textbf{J}_w(w,b) = -y_n\\textbf{x}_n + \\lambda \\textbf{w}\n",
    "$$\n",
    "$$\n",
    "\\nabla \\textbf{J}_b(w,b) = -y_n\n",
    "$$\n",
    "\n",
    "else:\n",
    "\n",
    "$$\n",
    "\\nabla \\textbf{J}_w(w,b) = \\lambda \\textbf{w}\n",
    "$$\n",
    "$$\n",
    "\\nabla \\textbf{J}_b(w,b) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X0.min(),X0.max())\n",
    "print(X1.min(),X1.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 100\n",
    "lam = 1./C\n",
    "\n",
    "def loss(X, y, w, b): \n",
    "    \"\"\"\n",
    "    X.shape = (2N, d), y.shape = (2N,), w.shape = (d,), b is a scalar \n",
    "    \"\"\"\n",
    "    z = X.dot(w) + b # shape (2N,)\n",
    "    yz = y*z\n",
    "    return #TODO: Define the loss function\n",
    "\n",
    "def grad(X, y, w, b):\n",
    "    z = X.dot(w) + b # shape (2N,)\n",
    "    yz = y*z         # element wise product, shape (2N,)\n",
    "    active_set = np.where(yz <= 1)[0] # consider 1 - yz >= 0 only \n",
    "    _yX = - X*y[:, np.newaxis]   # each row is y_n*x_n \n",
    "    grad_w = #TODO Calculate gradient of w\n",
    "    grad_b = #TODO Calculate gradient of b\n",
    "    return (grad_w, grad_b)\n",
    "\n",
    "def num_grad(X, y, w, b):\n",
    "    eps = 1e-10\n",
    "    gw = np.zeros_like(w)\n",
    "    gb = 0\n",
    "    for i in range(len(w)):\n",
    "        wp = w.copy()\n",
    "        wm = w.copy()\n",
    "        wp[i] += eps \n",
    "        wm[i] -= eps \n",
    "        gw[i] = (loss(X, y, wp, b) - loss(X, y, wm, b))/(2*eps)\n",
    "    gb = (loss(X, y, w, b + eps) - loss(X, y, w, b - eps))/(2*eps)\n",
    "    return (gw, gb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = .1*np.random.randn(X.shape[1])\n",
    "b = np.random.randn()\n",
    "(gw0, gb0) = grad(X, y, w, b)\n",
    "(gw1, gb1) = num_grad(X, y, w, b)\n",
    "print('grad_w difference = ', np.linalg.norm(gw0 - gw1))\n",
    "print('grad_b difference = ', np.linalg.norm(gb0 - gb1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmarginSVM_gd(X, y, w0, b0, eta):\n",
    "    w = w0\n",
    "    b = b0\n",
    "    it = 0 \n",
    "    while it < 10000:\n",
    "        it = it + 1\n",
    "        (gw, gb) = #TODO Get gradient from grad function\n",
    "        w -= eta*gw\n",
    "        b -= eta*gb\n",
    "        if (it % 1000) == 0:\n",
    "            print('iter %d' %it + ' loss: %f' %loss(X, y, w, b))\n",
    "    return (w, b)\n",
    "\n",
    "w0 = .1*np.random.randn(X.shape[1]) \n",
    "b0 = .1*np.random.randn()\n",
    "lr = 0.05\n",
    "(w_hinge, b_hinge) = softmarginSVM_gd(X, y, w0, b0, lr)\n",
    "print('w_hinge = ', w_hinge)\n",
    "print('b_hinge = ', b_hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myplot(X0, X1, w, b, filename, tit):\n",
    "    with PdfPages(filename) as pdf:\n",
    "        fig, ax = plt.subplots()\n",
    "        w0 = w[0]\n",
    "        w1 = w[1]\n",
    "        x1 = np.arange(-10, 10, 0.1)\n",
    "        y1 = -w0/w1*x1 - b/w1\n",
    "        y2 = -w0/w1*x1 - (b-1)/w1\n",
    "        y3 = -w0/w1*x1 - (b+1)/w1\n",
    "        plt.plot(x1, y1, 'k', linewidth = 3)\n",
    "        plt.plot(x1, y2, 'k')\n",
    "        plt.plot(x1, y3, 'k')\n",
    "\n",
    "        # equal axis and lim\n",
    "        plt.axis('equal')\n",
    "        plt.ylim(0, 4)\n",
    "        plt.xlim(2, 4)\n",
    "\n",
    "        # hide ticks \n",
    "        cur_axes = plt.gca()\n",
    "        cur_axes.axes.get_xaxis().set_ticks([])\n",
    "        cur_axes.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "        plt.xlabel('$x_1$', fontsize = 20)\n",
    "        plt.ylabel('$x_2$', fontsize = 20)\n",
    "        plt.title('Solution: ' + tit, fontsize = 20)\n",
    "\n",
    "        plt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = .8)\n",
    "        plt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = .8)\n",
    "        pdf.savefig()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot(X0, X1, w_hinge, b_hinge, 'svm_hinge.pdf', 'GD using Hinge Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solving SVM problem using Lagrange Duality and KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function:**\n",
    "$$\n",
    "\t\\textbf{J}(w,b,\\xi_n) = min_{\\textbf{w},b,\\xi} \\frac{1}{2}||\\textbf{w}||^2 + {C}\\sum \\xi_n\n",
    "$$\n",
    "$$\n",
    "s.t. y_n [\\textbf{w}^T\\textbf{x}_n +b ] \\geq 1 - \\xi_n\n",
    "$$ and $\\xi_n \\geq 0 $  while $C$ is constant and $ C> 0$\n",
    "\n",
    "**Lagrangian:**\n",
    "\n",
    "$$\n",
    "L(\\textbf{w},b,\\{\\xi_n\\},\\{\\alpha_n\\},\\{\\lambda_n\\}) = {C}\\sum \\xi_n +  \\frac{1}{2}||\\textbf{w}||^2 - \\sum_n \\lambda_n \\xi_n + \\sum_n \\alpha_n \\{ 1- y_n [\\textbf{w}^T\\textbf{x}_n +b ] - \\xi_n\\}\n",
    "$$\n",
    "\n",
    "$$\\alpha_n \\geq 0,\\lambda_n \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking derivatives of $L$ st $b$,$\\xi_n$, $\\textbf{w}$ respectively we have:\n",
    "$$\n",
    "\\sum_n \\alpha_ny_n = 0\n",
    "$$\n",
    "$$\n",
    "C -\\lambda_n - \\alpha_n = 0\n",
    "$$\n",
    "\n",
    "\n",
    "**and the solution for w**:\n",
    "$$\n",
    "\\textbf{w} = \\sum_n \\alpha_n y_n \\textbf{x}_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the dual problem to find $\\alpha$:** \n",
    "\n",
    "$$\n",
    "   max_{\\alpha} g(\\alpha_n, \\lambda_n) = \\sum \\alpha_n - \\frac{1}{2}\\sum_{m,n}y_my_n\\alpha_m\\alpha_n\\textbf{x}_m^T \\textbf{x}_n\n",
    "$$\n",
    "$$\n",
    "s.t. \\alpha_n > 0 , \\sum \\alpha_ny_n = 0, C- \\lambda_n - \\alpha_n = 0,  \\lambda_n \\geq 0\n",
    "$$\n",
    "and KKT we have:\n",
    "\n",
    "$\\xi_n = 0$ and $0 < \\alpha_n < C$ when $y_n [\\textbf{w}^T\\textbf{x}_n +b ] = 1$\n",
    "\n",
    "$\\xi_n > 0$ and $\\alpha_n  = C$ if $y_n [\\textbf{w}^T\\textbf{x}_n +b ] < 1$ \n",
    "\n",
    "**Solution for b:**\n",
    "$$\n",
    "b = y_n - \\sum_m \\alpha_m y_m \\textbf{x}_m^T \\textbf{x}_n\n",
    "$$\n",
    "\n",
    "If $0 < \\alpha_n < C$ and $y_n \\in \\{-1,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to https://xavierbourretsicotte.github.io/SVM_implementation.html for information about Solving a quadratic program using python\n",
    "\n",
    "Assume that matrix:\n",
    "$V = [y_1\\textbf{x}_1,y_2\\textbf{x}_2,...,y_n\\textbf{x}_n]$ and $K = V^TV$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Step 1: Finding alpha \n",
    "V = np.concatenate((X0, -X1), axis = 0) # V[n,:] = y[n]*X[n]\n",
    "\n",
    "# build K\n",
    "K = matrix(V.dot(V.T))\n",
    "p = matrix(-np.ones((2*N, 1)))\n",
    "\n",
    "# build A, b, G, h \n",
    "G = matrix(np.vstack((-np.eye(2*N), np.eye(2*N))))\n",
    "h = np.vstack((np.zeros((2*N, 1)), C*np.ones((2*N, 1))))\n",
    "h = matrix(np.vstack((np.zeros((2*N, 1)), C*np.ones((2*N, 1)))))\n",
    "A = matrix(y.reshape((-1, 2*N))) \n",
    "b = matrix(np.zeros((1, 1))) \n",
    "solvers.options['show_progress'] = False\n",
    "\n",
    "#  solving the dual problem (finding: alpha)\n",
    "sol = solvers.qp(K, p, G, h, A, b)\n",
    "al = np.array(sol['x']).reshape(2*N) # alpha vector \n",
    "\n",
    "# Step 2: finding w and b using above solution\n",
    "w_dual = # To Do; Caculate W using dual methods\n",
    "b_dual = # To Do; Caculate b using dual methods\n",
    "print('w_dual = ', w_dual)\n",
    "print('b_dual = ', b_dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot(X0, X1, w_dual, b_dual, 'svm_b_dual.pdf', 'GD using Duality and KKT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solving by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "C = 100 # lambda = 0.01\n",
    "clf = #TODO: Using SVC from sklearn to find w and b\n",
    "clf.fit(X, y) \n",
    "\n",
    "w_sklearn = clf.coef_.reshape(-1, 1)\n",
    "b_sklearn = clf.intercept_[0]\n",
    "\n",
    "print('w_sklearn = ', w_sklearn.T)\n",
    "print('b_sklearn = ', b_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot(X0, X1, w_sklearn, b_sklearn, 'svm_b_sklearn.pdf', 'GD using sklearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate non-linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "# XOR dataset and targets\n",
    "X = np.c_[(-1, -3),\n",
    "          (0, -2),\n",
    "          (1, -2),\n",
    "          (2, 1),\n",
    "          (3, 0),\n",
    "          (1.5, 1.4),\n",
    "          #---\n",
    "          (1, 3),\n",
    "          (0.5, -1),\n",
    "          (-1, 2),\n",
    "          (-2, -1.5),\n",
    "          (-2, 0),\n",
    "          (-1.5, -1.2)].T\n",
    "N = 6\n",
    "Y = [0] * N + [1] * N\n",
    "\n",
    "plt.plot(X[:N, 0], X[:N, 1], 'bs', markersize = 8)\n",
    "plt.plot(X[N:, 0], X[N:, 1], 'ro', markersize = 8)\n",
    "plt.axis('tight')\n",
    "x_min = -5\n",
    "x_max = 5\n",
    "y_min = -5\n",
    "y_max = 5\n",
    "plt.figure(fignum, figsize=(4, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observer the boundary using 3 Kernel Functions: Sigmoid, Polynomial of power, and rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fignum = 1\n",
    "# fit the model\n",
    "for kernel in ('sigmoid', 'poly', 'rbf'):\n",
    "    clf = svm.SVC(kernel=kernel, gamma=1, coef0 = 1)\n",
    "    clf.fit(X, Y)\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.figure(fignum, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
    "                    facecolors='None')\n",
    "    plt.plot(X[:N, 0], X[:N, 1], 'bs', markersize = 8)\n",
    "    plt.plot(X[N:, 0], X[N:, 1], 'ro', markersize = 8)\n",
    "    plt.axis('tight')\n",
    "    x_min = -5\n",
    "    x_max = 5\n",
    "    y_min = -5\n",
    "    y_max = 5\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.figure(fignum, figsize=(4, 3))\n",
    "    CS = plt.contourf(XX, YY, np.sign(Z), 200, cmap='jet', alpha = .2)\n",
    "    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "                    levels=[-.5, 0, .5])\n",
    "    plt.title(kernel, fontsize = 15)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    fignum = fignum + 1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
