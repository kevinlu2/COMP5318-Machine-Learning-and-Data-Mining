{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining \n",
    "\n",
    "## Tutorial 10 - Perceptron and Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 2, 2020**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To learn about Perceptron\n",
    "* To learn about simple NN\n",
    "\n",
    "**Instructions:**\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"lab11.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"lab11.ipynb\" file\n",
    "* Complete exercises in \"lab10.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "\n",
    "Lecturers: Nguyen Hoang Tran \n",
    "\n",
    "Tutors: Canh Dinh, Chen Chen, Claire Hardgrove, Fengxiang He, Henry Weld, Yixuan Zhang, Zhiyi Wang, Thomas Selvaraj."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2)\n",
    "\n",
    "means = [[-1, 0], [1, 0]]\n",
    "cov = [[.3, .2], [.2, .3]]\n",
    "N = 10\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "\n",
    "X = np.concatenate((X0, X1), axis = 0)\n",
    "y = np.concatenate((np.ones(N), -1*np.ones(N)))\n",
    "\n",
    "#fig,axs=plt.subplots(nrows,ncols,figsize=(width,height))\n",
    "plt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = .8)\n",
    "plt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = .8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has 2 classes in total. We will use perceptron algorithm to make a decision boundary for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In perception, we minimizes the number of errors on the training dataset follows:\n",
    "\n",
    "$$ \\epsilon = \\sum_n 1_{[y_n \\neq sign(w^tx_n)]}$$\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "For a randomly chosen data point $(x_n, y_n)$ make small changes to $w$ so that : $y_n = sign(w^tx_n)$:\n",
    "\n",
    "2 case:\n",
    "- If $y_n = sign(w^tx_n)$: Do nothing\n",
    "- If $y_n \\neq sign(w^tx_n)$: update w : $w_{t+1} = w_{t} + y_nx_n$ => we only update $w$ for misclassified point. The update process will be stopped when there is no misclassified point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Perception Algorithm from scratch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, X):    \n",
    "    '''\n",
    "    predict label of each row of X, given w \n",
    "    '''\n",
    "    return #ToDo: Finish predict output\n",
    "\n",
    "def perceptron(X, y, w_init):\n",
    "    w = w_init\n",
    "    w_hist = [w]\n",
    "    mis_points = [] # list of misclassified points\n",
    "    while True:\n",
    "        pred = predict(w, X)\n",
    "        mis_idxs = np.where(np.equal(pred, y) == False)[0]\n",
    "        num_mis = mis_idxs.shape[0]\n",
    "        if num_mis == 0:\n",
    "            return (w_hist, mis_points)\n",
    "        # random pick one misclassified point \n",
    "        random_id = np.random.choice(mis_idxs, 1)[0]\n",
    "        mis_points.append(random_id)\n",
    "        w = # TODO: finisish updating rule of w\n",
    "        w_hist.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(73)\n",
    "Xbar = np.concatenate((np.ones((2*N, 1)), X), axis = 1)\n",
    "d = Xbar.shape[1]\n",
    "w_init = np.random.randn(d)\n",
    "w_hist, m = perceptron(Xbar, y, w_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the change of boundary during the learning processs depends on number of update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages    \n",
    "def draw_line(plt, w):\n",
    "    w0, w1, w2 = w[0], w[1], w[2]\n",
    "    if w2 != 0:\n",
    "        x11, x12 = -100, 100\n",
    "        return plt.plot([x11, x12], [-(w1*x11 + w0)/w2, -(w1*x12 + w0)/w2], 'k')\n",
    "    else:\n",
    "        x10 = -w0/w1\n",
    "        return plt.plot([x10, x10], [-100, 100], 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = m \n",
    "if(len(m) >= 3):\n",
    "    ncols = 3\n",
    "nrows = math.ceil(len(m)/3)\n",
    "width = 4.5*ncols \n",
    "height = 3.5*nrows\n",
    "\n",
    "filename = 'pla_visualize1.pdf'\n",
    "with PdfPages(filename) as pdf: \n",
    "    plt.close('all')\n",
    "    fig,axs=plt.subplots(nrows,ncols,figsize=(width,height))\n",
    "    ids = range(len(m)+1)\n",
    "    for i, k in enumerate(ids[1:]):\n",
    "        #print(\"i,k\",i,k)\n",
    "        r = i//ncols \n",
    "        c = i%ncols \n",
    "        str0 = 'iter {}/{}'.format(i+1, len(ids)-1)\n",
    "        if(nrows > 1):\n",
    "            temp = axs[r, c]\n",
    "        else:\n",
    "            temp = axs[c]\n",
    "        if nrows > 1:\n",
    "            temp.set_title(str0)\n",
    "            temp.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = .8)\n",
    "            temp.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = .8)\n",
    "            temp.plot(0, 0, 'k.', markersize = 8, alpha = .8)\n",
    "            temp.axis([0 , 6, -2, 4])\n",
    "            draw_line(temp, w_hist[k])\n",
    "            wx, wy = w_hist[k][1], w_hist[k][2]\n",
    "            temp.annotate('', xy=(wx, wy), xytext=(0, 0),arrowprops=dict(arrowstyle=\"simple\", connectionstyle=\"arc3\", ec = 'green', fc = 'green'))\n",
    "        if k < nrows*ncols:\n",
    "            \n",
    "            # get misclassified point\n",
    "            xmis = X[m[k], 0] \n",
    "            ymis = X[m[k], 1]\n",
    "            \n",
    "            #circle around the misclassified point\n",
    "            circle = plt.Circle((xmis, ymis), 0.2, color='k', fill = False)\n",
    "            temp.add_artist(circle)\n",
    "            \n",
    "            #vector to xmis\n",
    "            temp.annotate('', xy=(xmis, ymis), xytext=(0, 0),\n",
    "                        arrowprops=dict(arrowstyle=\"simple\", connectionstyle=\"arc3\", ec = 'orange', fc = 'orange'))\n",
    "\n",
    "            if m[k] > 10:\n",
    "                #New w if the misclassified point is in red \n",
    "                temp.annotate('', xytext=(0, 0), xy=(wx - xmis, wy-ymis),\n",
    "                        arrowprops=dict(arrowstyle=\"simple\", connectionstyle=\"arc3\", ec = 'red', fc = 'red'))\n",
    "            else: # the misclassified point is in blue\n",
    "                temp.annotate('', xytext=(0, 0), xy=(wx + xmis, wy+ymis),\n",
    "                        arrowprops=dict(arrowstyle=\"simple\", connectionstyle=\"arc3\", ec = 'red', fc = 'red'))\n",
    "                \n",
    "        temp.axis('scaled')\n",
    "        temp.axis([-3, 3, -1.5, 2.5])\n",
    "    pdf.savefig(bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blue points: Class 1, Red points: Class -1\n",
    "\n",
    "\n",
    "**Green Vector** is $w_t$.\n",
    "\n",
    "**Red Vertor** is $w_{t+1}$\n",
    "\n",
    "The circled points are misclassified point ($x_i$)\n",
    "\n",
    "**Orange Vector** is xi\n",
    "\n",
    "- If $y_i = 1$ (blue), red vector = Sum(Green Vector, Orange Vector)\n",
    "- If $y_i = âˆ’1$, red vector = Sub(Green Vector, Orange Vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multilayer Neural Network and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a dataset has multipel classes and does not have the linear bondary betwen classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # number of points per class\n",
    "d0 = 2 # dimensionality\n",
    "C = 3 # number of classes\n",
    "X = np.zeros((N*C, d0)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*C, dtype='uint8') # class labels\n",
    "\n",
    "for j in range(C):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "# lets visualize the data:\n",
    "# plt.scatter(X[:N, 0], X[:N, 1], c=y[:N], s=40, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.plot(X[:N, 0], X[:N, 1], 'bs', markersize = 7, markeredgecolor = 'k');\n",
    "plt.plot(X[N:2*N, 0], X[N:2*N, 1], 'ro', markersize = 7, markeredgecolor = 'k');\n",
    "plt.plot(X[2*N:, 0], X[2*N:, 1], 'g^', markersize = 7, markeredgecolor = 'k');\n",
    "# plt.axis('off')\n",
    "plt.xlim([-1.5, 1.5])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_ticks([])\n",
    "cur_axes.axes.get_yaxis().set_ticks([])\n",
    "filename = 'EX.pdf'\n",
    "with PdfPages(filename) as pdf:\n",
    "    pdf.savefig(bbox_inches='tight')\n",
    "# plt.savefig('EX.png', bbox_inches='tight', dpi = 600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Consider a neural network with one hidden layer using Relu activation function and Softmax function at the end of the network\n",
    "<img src=\"Network.png\" alt=\"2 layers Network\" title=\"2 layers Network\" width=\"450\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feedforward step**\n",
    "$$ Z^{(1)} = W^{(1)T}X + B^{(1)} $$\n",
    "$$ A^{(1)} = max(Z^{(1)},0) $$\n",
    "$$ Z^{(2)} = W^{(2)T}A^{(1)} + B^{(2)} $$\n",
    "$$ Y =A^{(2)}   = softmax(Z^{(2)},0) $$\n",
    "\n",
    "$ A^{(1)} $ is output of the hidden layer after going through Relu activation function\n",
    "\n",
    "$Y$ is final output after going through softmax  function\n",
    "\n",
    "$W^{(1)}, B^{(1)}$ and $W^{(2)}, B^{(2)}$ are weight and bias of hidden layer and output layer respectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation step**\n",
    "$$ E^{(2)} = \\nabla Z^{(2)} = \\frac{1}{N} (A^{(2)} - Y) $$\n",
    "$$ \\nabla W^{(2)} = A^{(1)} E^{(2)T} , \\nabla B^{(2)} = \\sum_1^N e_n^{(2)} $$\n",
    "$$ E^{(1)} =  W^{(2)} E^{(2)} \\odot \\nabla f(Z^{(1)}) $$\n",
    "$$ \\nabla W^{(1)} = A^{(0)} E^{(1)T}  = X E^{(1)T} , \\nabla B^{(1)} = \\sum_1^N e_n^{(1)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in Z.\n",
    "    each row of Z is a set of scores.    \n",
    "    \"\"\"\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 1, keepdims = True))\n",
    "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    "    return A\n",
    "\n",
    "def crossentropy_loss(Yhat, y):\n",
    "    \"\"\"\n",
    "    Yhat: a numpy array of shape (Npoints, nClasses) -- predicted output \n",
    "    y: a numpy array of shape (Npoints) -- ground truth. We don't need to use \n",
    "    the one-hot vector here since most of elements are zeros. When programming \n",
    "    in numpy, we need to use the corresponding indexes only.\n",
    "    \"\"\"\n",
    "    id0 = range(Yhat.shape[0])\n",
    "    return -np.mean(np.log(Yhat[id0, y]))\n",
    "\n",
    "def mlp_init(d0, d1, d2):\n",
    "    \"\"\" \n",
    "    Initialize W1, b1, W2, b2 \n",
    "    d0: dimension of input data \n",
    "    d1: number of hidden unit \n",
    "    d2: number of output unit = number of classes\n",
    "    \"\"\"\n",
    "    W1 = 0.01*np.random.randn(d0, d1)\n",
    "    b1 = np.zeros(d1)\n",
    "    W2 = 0.01*np.random.randn(d1, d2)\n",
    "    b2 = np.zeros(d2)\n",
    "    return (W1, b1, W2, b2)\n",
    "\n",
    "def mlp_predict(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Suppose that the network has been trained, predict class of new points. \n",
    "    X: data matrix, each ROW is one data point.\n",
    "    W1, b1, W2, b2: learned weight matrices and biases \n",
    "    \"\"\"\n",
    "    Z1 = # TODO:    # shape (N, d1)\n",
    "    A1 = # TODO:   # shape (N, d1)\n",
    "    Z2 = # TODO:    # shape (N, d2)\n",
    "    return np.argmax(Z2, axis=1)\n",
    "\n",
    "def mlp_fit(X, y, W1, b1, W2, b2, eta):\n",
    "    loss_hist = []\n",
    "    for i in range(10000):\n",
    "        # feedforward \n",
    "        Z1 = # TODO:      # shape (N, d1)\n",
    "        A1 = # TODO:    # shape (N, d1)\n",
    "        Z2 = # TODO:     # shape (N, d2)\n",
    "        Yhat = # TODO: # shape (N, d2)\n",
    "        \n",
    "        if i %1000 == 0: # print loss after each 1000 iterations\n",
    "            loss = crossentropy_loss(Yhat, y)\n",
    "            print(\"iter %d, loss: %f\" %(i, loss))\n",
    "            loss_hist.append(loss)\n",
    "\n",
    "        # back propagation\n",
    "        id0 = range(Yhat.shape[0])\n",
    "        Yhat[id0, y] -=1 \n",
    "        E2 =  # TODO:               # shape (N, d2)\n",
    "        dW2 = # TODO:     # shape (d1, d2)\n",
    "        db2 = # TODO: # shape (d2,)\n",
    "        E1 =  # TODO:     # shape (N, d1)\n",
    "        E1[Z1 <= 0] = 0            # gradient of ReLU, shape (N, d1)\n",
    "        dW1 = # TODO:      # shape (d0, d1)\n",
    "        db1 = # TODO:# shape (d1,)\n",
    "\n",
    "        # Gradient Descent update\n",
    "        W1 += -eta*dW1\n",
    "        b1 += -eta*db1\n",
    "        W2 += -eta*dW2\n",
    "        b2 += -eta*db2\n",
    "    return (W1, b1, W2, b2, loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = 2\n",
    "d1 = h = 500 # size of hidden layer\n",
    "d2 = C = 3\n",
    "eta = 1 # learning rate\n",
    "# initialize parameters randomly\n",
    "(W1, b1, W2, b2) = mlp_init(d0, d1, d2)\n",
    "(W1, b1, W2, b2, loss_hist) =mlp_fit(X, y, W1, b1, W2, b2, eta)\n",
    "\n",
    "y_pred = mlp_predict(X, W1, b1, W2, b2)\n",
    "acc = 100*np.mean(y_pred == y)\n",
    "print('training accuracy: %.2f %%' % acc)\n",
    "\n",
    "# Visualize results\n",
    "\n",
    "xm = np.arange(-1.5, 1.5, 0.025)\n",
    "xlen = len(xm)\n",
    "ym = np.arange(-1.5, 1.5, 0.025)\n",
    "ylen = len(ym)\n",
    "xx, yy = np.meshgrid(xm, ym)\n",
    "xx1 = xx.ravel().reshape(1, xx.size)\n",
    "yy1 = yy.ravel().reshape(1, yy.size)\n",
    "\n",
    "\n",
    "X0 = np.vstack((xx1, yy1)).T\n",
    "Z = mlp_predict(X0, W1, b1, W2, b2)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.clf()\n",
    "CS = plt.contourf(xx, yy, Z, 200, cmap='jet', alpha = .3)\n",
    "\n",
    "plt.plot(X[:N, 0], X[:N, 1], 'bs', markersize = 7, markeredgecolor = 'k');\n",
    "plt.plot(X[N:2*N, 0], X[N:2*N, 1], 'go', markersize = 7, markeredgecolor = 'k');\n",
    "plt.plot(X[2*N:, 0], X[2*N:, 1], 'r^', markersize = 7, markeredgecolor = 'k');\n",
    "\n",
    "# plt.axis('off')\n",
    "plt.xlim([-1.5, 1.5])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_ticks([])\n",
    "cur_axes.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "str0 = '\\#hidden units = ' + str(d1) + ', accuracy =' + str(acc) + '\\%'\n",
    "plt.title(str0, fontsize = 15)\n",
    "\n",
    "filename = 'ex_res'+ str(d1) + '.pdf'\n",
    "with PdfPages(filename) as pdf:\n",
    "    pdf.savefig(bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
