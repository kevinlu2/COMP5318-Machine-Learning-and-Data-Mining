{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining: Assignment 1\n",
    "<div style=\"text-align: right\"> Due: Wednesday 14 Oct 2020 11:59PM </div>\n",
    "\n",
    "The goal of this assignment is to build a classifier to classify some grayscale images of the size 28x28 into a set of categories. The dimension of the original data is large, so you need to be smart on which method you gonna use and perhaps perform a pre-processing step to reduce the amount of computation. Part of your marks will be a function of the performance of your classifier on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware and software specifications <br />\n",
    "MacBook Pro (Retina, 13-inch, Mid 2014) <br />\n",
    "Processor: 2.8 GHz Intel Core i5 <br />\n",
    "Memory: 8 GB 1600 MHz DDR3 <br />\n",
    "Graphics: Intel Iris 1536 MB <br />\n",
    "OS: macOS Mojave Version 10.14.6 <br />\n",
    "\n",
    "Code is written in Visual Studio Code with Python Extension for Visual Studio Code by Microsoft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images_training.h5', 'labels_training.h5']\n",
      "['.DS_Store', 'images_testing.h5', 'labels_testing_2000.h5']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from bisect import bisect\n",
    "from scipy.spatial import distance\n",
    "from math import exp, sqrt, pi\n",
    "print(os.listdir(\"./Input/train\"))\n",
    "print(os.listdir(\"./Input/test\"))\n",
    "# train_files = [name for name in os.listdir(\"./Input/train\") if not name.endswith('DS_Store')]\n",
    "# test_files = [name for name in os.listdir(\"./Input/test\") if not name.endswith('DS_Store')]\n",
    "# print(train_files)\n",
    "# print(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('./Input/train/images_training.h5','r') as H:\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./Input/train/labels_training.h5','r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "\n",
    "# using H['datatest'], H['labeltest'] for test dataset.\n",
    "print(data_train.shape,label_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784) (2000,)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('./Input/test/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./Input/test/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])\n",
    "\n",
    "# using H['datatest'], H['labeltest'] for test dataset.\n",
    "print(data_test.shape,label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "<div style=\"text-align: right\"> Sourced: Tutorial 2 - Matrix Decomposition</div>\n",
    "<div style=\"text-align: right\"> Sourced: https://stats.stackexchange.com/questions/125172/pca-on-train-and-test-datasets-should-i-run-one-pca-on-traintest-or-two-separa </div>\n",
    "<div style=\"text-align: right\"> https://towardsdatascience.com/pca-with-numpy-58917c1d0391 </div>\n",
    "<div style=\"text-align: right\"> https://stackoverflow.com/questions/10818718/principal-component-analysis </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Eigenvectors and Eigenvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784)\n",
      "First 20 Eigenvalues: \n",
      " [19.8611004  12.10997382  4.1078177   3.3719857   2.61461635  2.35693788\n",
      "  1.61184549  1.28149922  0.92593176  0.89463432  0.67365696  0.62224642\n",
      "  0.52434522  0.44943814  0.41495554  0.4023021   0.37964251  0.36276613\n",
      "  0.31522305  0.31177036] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data_train   #Create copy of training data\n",
    "data = data - np.mean(data, axis=0) #Centre observations by zero mean\n",
    "print(data.shape)\n",
    "covariance_matrix = np.cov(data.T)  #Create the covariance matrix between the 784 features\n",
    "#print(covariance_matrix)\n",
    "eig_val, eig_vec = np.linalg.eig(covariance_matrix) #From linear algebra, retrive the eiganvalue and eigan vectors from the covariance matrix\n",
    "print(\"First 20 Eigenvalues: \\n\", eig_val[:20], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking Principal Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Variance Explained: \n",
      " [29.077053310731294, 17.72924699012503, 6.013928330120427, 4.936655363514209, 3.8278512994969107, 3.4506048031981504, 2.3597744497586266, 1.876140819639972, 1.3555828542550032, 1.309762764368793] \n",
      "\n",
      "First 10 Cummulative Variance Explained: \n",
      " [29.07705331 46.8063003  52.82022863 57.75688399 61.58473529 65.0353401\n",
      " 67.39511455 69.27125537 70.62683822 71.93660099] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "variance_explained = [] #Declare list to hold all variances explaine for each feature.\n",
    "for i in eig_val:\n",
    "    variance_explained.append((i/sum(eig_val))*100)    #Get proportion of eiganvalue of each feature and multiply by 100 to get percentage\n",
    "print(\"First 10 Variance Explained: \\n\", variance_explained[:10], \"\\n\") \n",
    "\n",
    "cumulative_variance_explained = np.cumsum(variance_explained)   #Gets list of cummulative variancces\n",
    "print(\"First 10 Cummulative Variance Explained: \\n\", cumulative_variance_explained[:10], \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Cumulative explained variance to find elbow point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Explained variance vs Number of components')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "comp = bisect(cumulative_variance_explained, 95) #Number of components at 95% explained variance\n",
    "sns.lineplot(x = np.arange(data.shape[1]), y=cumulative_variance_explained) #Plot numbercomponents by cummulative variance\n",
    "plt.axvline(comp, c='m', linestyle='--', alpha=0.6) #abline to show where we are cutting for 95% variance explained\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"Explained variance vs Number of components\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the diagram above, the number of components for 95% variance explained is 187 components. Below we set this globablly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components for 95% Explained Variance: 187\n"
     ]
    }
   ],
   "source": [
    "n_component = comp\n",
    "print(\"Number of components for 95% Explained Variance:\", n_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Data Onto Lower-Dimensional Linear Subspace\n",
    "</br>\n",
    "We now will create our projection matrix by reducing our array of eiganvectors by the number of components found above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project onto training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 187)\n"
     ]
    }
   ],
   "source": [
    "projection_matrix = (eig_vec.T[:][:n_component]).T\n",
    "X_train_pca = data.dot(projection_matrix)\n",
    "print(X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project on to test data\n",
    "To keep consistency the same projecetion matrix is applied onto the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 187)\n"
     ]
    }
   ],
   "source": [
    "data = data_test\n",
    "data = data - np.mean(data, axis=0)\n",
    "X_test_pca = data.dot(projection_matrix)\n",
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "Split training data into train and validaiton sets on 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pct_index = int(0.7 * X_train_pca.shape[0])\n",
    "# X_train, X_val = X_train_pca[:train_pct_index], X_train_pca[train_pct_index:]\n",
    "# y_train, y_val = label_train[:train_pct_index], label_train[train_pct_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Classificaiton Algorithm\n",
    "<div style=\"text-align: right\"> Sourced: COMP5318 Tutorial 5 - Classification I </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_knn(X, y, K, X_q, distance_method):\n",
    "    if distance_method == \"squared\" :   #Calculate distance between X_q and each training point\n",
    "        dis = ((X - X_q)**2).sum(axis=1) #Sum the distance of entire row, in other words by columns.\n",
    "    elif distance_method =='euclidean':\n",
    "        dis = np.sqrt(((X - X_q)**2).sum(axis=1))\n",
    "    elif distance_method == \"manhattan\":\n",
    "        dis = (X - X_q).sum(axis=1)\n",
    "    else:\n",
    "        raise ValueError('Undetermined distance')\n",
    "    arg_ascending = np.argsort(dis)     #Sort distanc matrix\n",
    "    return stats.mode(y[arg_ascending[:K]]).mode    #Take the neighbour that occurs the most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "Find best k method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# method = \"squared\"\n",
    "# k_list = np.arange(5, 13)\n",
    "# score_list = []\n",
    "# for k in k_list:    #iterate through the range of k neighbours\n",
    "#     y_pred = np.empty((len(X_val)))    #Declare empty arrary to hold predictions\n",
    "#     print(\"neighbour\", k)\n",
    "#     for i in range(len(y_val)):    #Iterate each sample\n",
    "#         y_pred[i]= calc_knn(X_train, y_train, k, X_val[i,:],method)    #Get prediction for given k\n",
    "#     score_list.append(np.mean(y_pred == y_val))    #append score into a score list\n",
    "# score_list = np.array(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot scores for each k and retrieve k with highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# n_comp_list = np.arange(5, 13)\n",
    "# ax.plot(k_list, score_list)\n",
    "# ax.axvline(n_comp_list[np.argmax(score_list)], c='m', linestyle='--', alpha=0.6)\n",
    "# ax.set_xlabel('# Neighbors')\n",
    "# ax.set_ylabel('Accuracy')\n",
    "# ax.set_xticks(k_list)\n",
    "# ax.set_title('Finding best k neighbours')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the k that yields the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = k_list[np.argmax(score_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# y_pred = np.empty((len(X_val)))\n",
    "# for i in range(len(y_val)):\n",
    "#     y_pred[i] = calc_knn(X_train, y_train, k, X_val[i,:], method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = y_val\n",
    "# y_pred = y_pred\n",
    "\n",
    "# y_true = pd.Series(y_true, name='Actual')\n",
    "# y_pred = pd.Series(y_pred, name='Predicted')\n",
    "# confusion_matrix = pd.crosstab(y_true, y_pred)\n",
    "# print(confusion_matrix)\n",
    "# kNN_train_acc = np.sum(y_true == y_pred[:y_true.shape[0]])/len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Test Dataset\n",
    "<div style=\"text-align: right\"> Sourced: COMP5318 Tutorial 5 - Classification I </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.3 s, sys: 7.84 s, total: 52.2 s\n",
      "Wall time: 53.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 6\n",
    "method = \"squared\"\n",
    "knn_y_pred = np.empty((len(X_test_pca)))\n",
    "for i in range(len(label_test)):\n",
    "    knn_y_pred[i] = calc_knn(X_train_pca, label_train, k, X_test_pca[i,:], method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0   All\n",
      "Actual                                                           \n",
      "0          162    0    5    4    3    1   12    0    5    0   192\n",
      "1            0  182    1    1    0    0    0    0    0    0   184\n",
      "2            5    1  161    4   20    0   15    0    0    0   206\n",
      "3            8    1    4  170   13    0   10    0    1    0   207\n",
      "4            1    0   29   13  160    0   17    0    0    0   220\n",
      "5            0    0    0    0    0  163    0   17    0   10   190\n",
      "6           36    1   24    4   14    0  107    0    4    0   190\n",
      "7            0    0    0    0    0    3    0  182    0    7   192\n",
      "8            0    0    2    1    2    0    2    0  219    1   227\n",
      "9            0    0    0    0    0    1    0    9    0  182   192\n",
      "All        212  185  226  197  212  168  163  208  229  200  2000\n"
     ]
    }
   ],
   "source": [
    "y_true = label_test\n",
    "y_pred = knn_y_pred[:y_true.shape[0]]\n",
    "\n",
    "y_true = pd.Series(y_true, name='Actual')\n",
    "y_pred = pd.Series(y_pred, name='Predicted')\n",
    "knn_confusion = pd.crosstab(y_true, y_pred,margins=True)\n",
    "print(knn_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_true == y_pred[:y_true.shape[0]])/len(y_true)\n",
    "\n",
    "knn_accuracy = np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes Classifier\n",
    "<div style=\"text-align: right\"> Sourced: COMP5318 Tutorial 5 - Classification I </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly for each class in our test set, the mean and variance is calculated for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_label_stats(X, y):\n",
    "#     labels = set(y)     #Get distinct classes from our test set\n",
    "#     label_stats = dict()    #Declare the dictionary that will hold a list of statistics\n",
    "#     for label in labels:    #Loop to iterate each class\n",
    "#         x_labels = X[y == label]    #Retreive the observations(x) that fall corresponds to the current class\n",
    "#         feature_stats = []      #Declare list to hold the statistic\n",
    "#         for i in range(X.shape[1]): #Iterate each row of our obeservations\n",
    "#             feature_x = x_labels[:,i]   #Select the column\n",
    "#             feature_stats.append([np.mean(feature_x), np.var(feature_x)])   #Append results to our list\n",
    "#         label_stats[label] = feature_stats  #Set the value of our class to the list of statistics\n",
    "#     return label_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The priors probability is calculated by finding the proportion of each class in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prior(X, y):    #Get the prior probabilty of our classes\n",
    "#     labels = set(y)\n",
    "#     prior = dict()      #Declare dictionary where key is our class and value its proportion\n",
    "#     for label in labels:\n",
    "#         prior[label] = float(len(y[y == label])) / len(y)   #Find number of labels in our dataset and divide by dataset length\n",
    "#     return prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the gaussian formula to get our conditional probabilities\n",
    "\\begin{equation}\n",
    "     \\frac{1}{{ \\sqrt {2\\pi \\sigma^{2}} }}e^{\\frac{{ x- \\mu }}{2 \\sigma^{2}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_p_given_c(Xq, mean, var):\n",
    "#     return  exp( -((Xq-mean)**2 / (2 * var )) ) / (sqrt(2 * pi * var))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the probability of each image for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_posterior(label_stats, row, prior, labels):\n",
    "#     prob = dict()   #Declare dictionaly to hold our probabilities\n",
    "#     for label in labels:    #Interate through our classes\n",
    "#         # prob[label] = prior[label]\n",
    "#         likelihood = prior[label]   #Set the current likelihood as our prior probability\n",
    "#         for i in range(row.shape[0]):\n",
    "#             mean, var = label_stats[label][i][0], label_stats[label][i][1]  #For each feature in our image we calculate the proability of each feature in the current class.\n",
    "#             likelihood *= get_p_given_c(row[i], mean, var)\n",
    "#         prob[label] = likelihood    #Append the posterier to our dicitonry\n",
    "#     return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is made by taking the class with the highest posterior probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_predictions(posterior):\n",
    "#     predictions = []\n",
    "#     for row in posterior:   #Interate through our posterior dictionary where key is the label and value is the posterioer.\n",
    "#         predictions.append(max(row, key=row.get))   #Return the key (class) that has the highest value (posterior)\n",
    "#     return predictions  #Return an arrary of our predicitons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fucntion below incorporates all funcitons above to make the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def NB_Classifier(X_train, y_train, X_test):\n",
    "#     #Train Data\n",
    "#     label_stats = get_label_stats(X_train, y_train) #Get statistics\n",
    "#     prior = get_prior(X_train, y_train) #Gets our priors\n",
    "#     posteriors = []\n",
    "#     for X_q in X_test:\n",
    "#         posteriors.append(get_posterior(label_stats, X_q, prior, label_set))    #Get posteriers of each row in our new data set\n",
    "#     y_pred = get_predictions(posteriors)    #Get prediciton for our new set\n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Train Model and predict on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# label_set = set(y_train)\n",
    "# y_pred = NB_Classifier(X_train, y_train, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = pd.Series(y_val, name='Actual')\n",
    "# y_pred = pd.Series(y_pred, name='Predicted')\n",
    "# confusion_matrix = pd.crosstab(y_true, y_pred)\n",
    "# print(confusion_matrix)\n",
    "# nb_train_acc = np.mean(y_pred == y_true)\n",
    "# print(\"\\nAccuracy:\", np.mean(y_pred == y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on Test Dataset\n",
    "<div style=\"text-align: right\"> Sourced: COMP5318 Tutorial 5 - Classification I </div>\n",
    "Prediciton is made on whole training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# nb_y_pred = NB_Classifier(X_train_pca, label_train, X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = pd.Series(label_test, name='Actual')\n",
    "# y_pred = pd.Series(nb_y_pred[:2000], name='Predicted')\n",
    "# nb_confusion = pd.crosstab(y_true, y_pred,margins=True)\n",
    "# print(nb_confusion)\n",
    "# print(\"\\nAccuracy:\", np.mean(y_pred == y_true))\n",
    "# nb_accuracy = np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinominal Logistic Regression Classifier\n",
    "<div style=\"text-align: right\"> Sourced: COMP5318 Tutorial 8 - Multinomial Logistic Regression </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multinominal Logistic Regression a softmax function is used to replace the sigmoid funciton.\n",
    "$$\n",
    "h_w(x) = p(y = c|x;w_1,...,w_c) = \\frac{\\exp(w_c^Tx)}{\\sum_{c=1}^C \\exp(w_c^Tx)} \n",
    "%- y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j = \\Pr(c = 1|x) - y_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax(Z): \n",
    "#     exp_z = np.exp(Z)\n",
    "#     return exp_z / exp_z.sum(axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Gradient\n",
    "$$\n",
    "w_j \\leftarrow w_j - \\alpha (\\sum_{i=1}^n(h_w(x^{i}) - y^{i})x^{i} + \\lambda w_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax_grad(X, y, W):\n",
    "#     A = softmax(X.dot(W))    # shape of (N, C)\n",
    "#     id0 = range(X.shape[0])  # number of train data\n",
    "#     A[id0, y] -= 1           # A - Y, shape of (N, C)\n",
    "#     return X.T.dot(A)/X.shape[0] #+ (l/2) * np.sum(W) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loss\n",
    "\\begin{equation}\n",
    "    \\text{loss}(\\mathbf{w}) = -l(\\mathbf{w}) = -\\sum_{k=1}^{C} 1\\{y=k\\}\\big( \\log(\\frac{\\exp(w_c^Tx)}{\\sum_{c=1}^C \\exp(w_c^Tx)}\\big)\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\"> Sourced: http://web.stanford.edu/~jurafsky/slp3/5.pdf</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax_loss(inputs, targets, weights):\n",
    "#     A = softmax(inputs.dot(weights))\n",
    "#     id0 = range(inputs.shape[0])\n",
    "#     return -np.mean(np.log(A[id0, targets]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our week 8 tutorial, fitting the multi-nominal logistic regression by batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # building learning fuction using softmax gradient decent\n",
    "# def softmax_fit(inputs, targets, weights, lr, epoches, batch_size):\n",
    "#     tol = 1e-5\n",
    "#     weights_prev = weights.copy()\n",
    "#     loss_hist = [softmax_loss(inputs, targets, weights)] # store history of loss \n",
    "#     N = inputs.shape[0]\n",
    "#     nbatches = int(np.ceil(float(N)/batch_size)) #number of batches for given batch size\n",
    "#     for epoch in range(epoches): \n",
    "#         mix_ids = np.random.permutation(N) # mix data \n",
    "#         for i in range(nbatches):\n",
    "#             # get the i-th batch\n",
    "#             batch_ids = mix_ids[batch_size*i : min(batch_size*(i+1), N) ] \n",
    "#             X_batch, y_batch = inputs[batch_ids], targets[batch_ids]\n",
    "#             weights -= lr * softmax_grad(X_batch, y_batch, weights) # update gradient descent \n",
    "#         loss_hist.append(softmax_loss(inputs, targets, weights))\n",
    "#         if np.linalg.norm(weights - weights_prev)/weights.size < tol:\n",
    "#             break \n",
    "#         weights_prev = weights.copy()\n",
    "#     return weights, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictins are made by taking the highest probability along each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prediction(weights, inputs):\n",
    "#     A = softmax(inputs.dot(weights)) #Use trained weights to find probabilites on test data.\n",
    "#     return np.argmax(A, axis = 1)   #Return classes with highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# C=len(np.unique(y_val))\n",
    "# weights = np.random.randn(X_train.shape[1], C)\n",
    "# weights, loss_hist = softmax_fit(X_train, y_train, weights, batch_size = 10, epoches = 100, lr = 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss against epoches to find optimal number of epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_hist)\n",
    "# plt.xlabel('number of epoches', fontsize = 13)\n",
    "# plt.ylabel('loss', fontsize = 13)\n",
    "# plt.tick_params(axis='both', which='major', labelsize=13)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can see from graph that there is a sharp elbow point. I have selected 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix of training data agaist validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = get_prediction(weights,X_val)\n",
    "# y_true = pd.Series(y_val, name='Actual')\n",
    "# y_pred = pd.Series(y_pred, name='Predicted')\n",
    "# confusion_matrix = pd.crosstab(y_true, y_pred)\n",
    "# print(confusion_matrix)\n",
    "# logit_train_acc = np.mean(y_pred == y_true)\n",
    "# print(\"\\nAccuracy:\", np.mean(y_pred == y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit whole data and predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# C=len(np.unique(label_train))\n",
    "# weights = np.random.randn(X_train_pca.shape[1], C)\n",
    "# weights, loss_hist = softmax_fit(X_train_pca, label_train, weights, batch_size = 1, epoches = 10, lr = 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# logist_y_pred = get_prediction(weights,X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = pd.Series(label_test, name='Actual')\n",
    "# y_pred = pd.Series(logist_y_pred[:2000], name='Predicted')\n",
    "# logist_confusion = pd.crosstab(y_true, y_pred,margins=True)\n",
    "# print(logist_confusion)\n",
    "# print(\"\\nAccuracy:\", np.mean(y_pred == y_true))\n",
    "# logist_accuracy = np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Anaylsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"kNN Accuracy:\", knn_accuracy)\n",
    "# knn_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
    "# nb_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Multinominal Logistic Accuracy:\", logist_accuracy)\n",
    "# logist_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_comparison = pd.DataFrame({\"Train Accuracy\":[kNN_train_acc, nb_train_acc, logit_train_acc],\"Test Accuracy\":[knn_accuracy,nb_accuracy,logist_accuracy]})\n",
    "# score_comparison.index = ['kNN', 'Naive Bayes', 'Multi-Logistic']\n",
    "# score_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output best prediction to Output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('Output/predicted_labels.h5','w') as H:\n",
    "    H.create_dataset('Output',data=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
